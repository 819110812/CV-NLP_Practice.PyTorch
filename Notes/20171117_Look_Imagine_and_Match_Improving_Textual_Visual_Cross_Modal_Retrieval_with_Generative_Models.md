# [Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models](https://arxiv.org/abs/1711.06420)

# 所见所想所找－基于生成模型的跨模态检索

## 摘要
视觉-文本跨模态检索已经成为计算机视觉和自然语言处理领域结合的一个热点。__对于跨模态检索而言，如何学到合适的特征表达非常关键__。本文提出了一种基于生成模型的跨模态检索方法，该方法可以学习跨模态数据的高层次特征相似性，以及目标模态上的局部相似性。本文通过大量的实验证明了所提出的方法可以准确地匹配图像和文本，并且在MSCOCO以及Flickr30K的数据集上都取得了state-of-the-art的效果。

## 引言
我们已经进入到了一个大数据时代，不同模态的数据例如文本、图像等正在以爆炸性的速度增长。这些异质的模态数据也给用户的搜索带来了挑战。对于文本-视觉的跨模态表示，常见的方法就是首先每个模态的数据编码成各自模态的特征表示，再映射到一个共同空间内。通过ranking loss来对其进行优化，使得相似的图像-文本对映射出的特征向量之间的距离小于不相似的图像-文本对之间的距离。_尽管这种方法学习出的特征可以很好地描述多模态数据高层语义，但是没有充分地挖掘图像的局部相似度和句子的句子层次相似度。例如文本检索图片时，我们会更多地关注图片的颜色、纹理以及布局等细节信息。而仅仅进行高层次特征匹配，显然无法考虑到局部的相似度_。

本文的想法来源于对人的思维的思考。对于人来说，给定一段文字描述去检索匹配的图像，一名训练有素画家可以比普通人找到更匹配的图像，那是因为画家知道预期的图片是什么样；类似，给一幅图片去检索匹配的文字描述，一名作家也往往会给出比普通人更好的描述。我们把这种对检索目标有预期的过程称为--”Imagine”或者“脑补”。因此，我们提出了一种基于生成模型的跨模态特征学习框架（generative cross-modal feature learning framework，GXN），下图展示了本文的思想：

![](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/d1681b73137b9c6935331551bffa36bd95bf6959/1-Figure1-1.png)

我们把原来的Look和Match变成了三个步骤：Look,Imagine和Match，也称为”所看所想所找”。Look叫“所看”，“看”是理解，实际就是提取特征。Imagine叫“所想”，根据“所看”去“脑补”预期的匹配结果, 也就是从得到的局部特征去生成目标模态的数据；Match也叫“所找”，根据生成/脑补的结果进行局部层次（sentence-level/pixel-level）匹配和高层次语义特征匹配。

## 方法
GXN包括三个模块：多模态特征表示（上部区域）；图像-文本生成特征学习（蓝色路径）和文本-图像生成对抗特征学习（绿色路径）。

![](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/d1681b73137b9c6935331551bffa36bd95bf6959/3-Figure2-1.png)

- 第一个部分（上部区域）和基本的跨模态特征表示做法类似，将不同模态的数据映射到共同空间。这里包括一个图像编码器$\text{CNN}_{\text{Enc}}$和两个句子编码器$\text{RNN}_{\text{Enc}}^h$和$\text{RNN}_{\text{Enc}}^l$。之所以分开2个句子编码器，是便于学到不同层次的特征。其中，$(v_h,t_h)$是高层语义特征而$(v_l,t_l)$作为局部层次的特征。这里的局部层次特征是通过生成模型学习得到的。
- 第二部分（蓝色路径）从底层视觉特征$v_l$生成一个文本描述。包括一个图像编码器$\text{CNN}_{\text{Enc}}$和一个句子解码器$\text{RNN}_{\text{Dec}}$。这里计算损失时我们结合了增强学习的思想，通过奖励的方式来确保生成句子和真实句子之间具有最大的相似度。
- 第三部分（绿色路径）通过使用一个cGAN从文本特征$t_l$中生成一幅图像，包括一个生成器$\text{CNN}_{\text{Dec}}$和一个判别器$D_i$。判别器用来区分基于文本生成的图像与真实图像。

最终，我们通过两路的跨模态特征生成学习学习到更好的跨模态特征表示。在测试时，我们只需要计算$\{v_h,t_h\}$和$\{v_l,t_l\}$之间的相似度来进行跨模态检索。

## 实验
本文提出的方法在MSCOCO数据集上和目前前沿的方法进行比较，并取得了state-of-the-art的结果。
![](https://ask.qcloudimg.com/http-save/yehe-1565119/9gqdok2m4v.jpeg?imageView2/0/w/1620)

# 总结
本文创新性地将图像-文本生成模型和文本-图像生成模型引入到传统的跨模态表示中，使其不仅能学习到多模态数据的高层的抽象表示，还能学习到底层的表示。显著超越state-of-the-art方法的表现证实了该方法的有效性。