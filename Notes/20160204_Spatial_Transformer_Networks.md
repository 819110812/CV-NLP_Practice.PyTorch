# [Spatial Transformer Networks](https://arxiv.org/pdf/1506.02025.pdf)

This paper is released by DeepMind, it aims at boosting the geometric invariance of CNNs in a very elegant way.


## Spatial Transformer networks


The goal of [spatial transformers](http://arxiv.org/abs/1506.02025) is to add to your base network a layer able to perform an <b>explicit geometric transformation</b> on an input. The parameters of the transformation are learnt thanks to the standard backpropagation algorithm, meaning there is no need for extra data or supervision.

<center>
<img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/spatial-transformer-structure.png" width=500> </image></center>


The layer is composed of 3 elements:
1. The <font style="color:red">localization network</font> takes the original image as an input and outputs the parameters of the transformation we want to apply.
2. The <font style="color:red">grid generator</font> generates a grid of coordinates in the input image corresponding to each pixel from the output image.
3. <font style="color:red">The sampler</font> generates the output image using the grid given by the grid generator.


As an example, here is what you get after training a network whose first layer is a ST:
<center><img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/st-mnist.png"></img><center>

On the left you see the input image. In the middle you see which part of the input image is sampled. On the right you see the Spatial Transformer output image.

## Interpretation

### At training time

Here the goal is to visualize how the Spatial Transformer behaves during training. In the animation below, you can see:
- on the left the original image used as input,
- on the right the transformed image produced by the Spatial Transformer,
- on the bottom a counter that represents training steps (0 = before training, 10/10 = end of epoch 1).


<center><img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/epoch_evolution.gif"></img></center>

<font size=2>Note: the white dots on the input image show the corners of the part of the image that is sampled. Same applies below.</font>

As expected, we see that during the training, the Spatial Transformer learns to focus on the traffic sign, learning gradually to remove background.

### Post-training
Here the goal is to visualize the ability of the Spatial Transformer (once trained) to produce a stable output even though the input contains geometric noise.

For the record the GTSRB dataset has been initially generated by extracting images from video sequences took while approaching a traffic sign.

The animation below shows for each image of such a sequence (on the left) the corresponding output of the Spatial Transformer (on the right).

<center><img src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/moving_evolution.gif"></img></cneter>

We can see that even though there is an important variability in the input images (scale and position in the image), the output of the Spatial Transformer remains almost static.

This confirms the intuition we had on how the Spatial Transformer simplifies the task for the rest of the network: learning to only forward the interesting part of the input and removing geometric noise.

The Spatial Transformer learned these transformations in an end-to-end fashion, without any modification to the backpropagation algorithm and without any extra annotations.
