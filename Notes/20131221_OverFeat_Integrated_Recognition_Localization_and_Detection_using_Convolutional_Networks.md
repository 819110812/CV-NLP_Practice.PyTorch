## [OverFeat:Integrated Recognition, Localization and Detection using Convolutional Networks](http://arxiv.org/abs/1312.6229)

## ***OverFeat笔记***

文章链接：http://arxiv.org/abs/1312.6229
源码：http://cilvr.nyu.edu/doku.php?id=software:overfeat:start

## ***一句话概括本文内容***

用CNN解决了ImageNet的***分类和定位的问题***，获得了2013年该项目的冠军。

## ***本文核心思想***

有两点，***其中第二点是亮点***：
1. 特征共享：首先在分类问题上训练出来模型，然后Fix住卷积层，来对定位模型进行fine tuning，最后对应1k个类别做了1k个定位的模型。
2. “快速”滑动窗口：为何要用滑动窗口？对于分类，通过多视角和多尺度来提高分类的置信度；对于定位，解决了多物体以及多尺度的问题。如何提高滑动窗口的效率？不在原始图片上做滑动窗口，而是在最后一个pooling层上面做滑动窗口。

## ***模型***

基本同AlexNet，主要是缩小了前两层的卷积核和步长，使得神经元从60m变成了144m。本质上是加强了网络的表达能力，训练时间会变长，但是准确率会升高。

## ***简述流程***

1. 初始化模型后，进来一张图片，利用滑动窗口技术提取出来多个patch。
2. 对于每个patch，用分类模型确定好类别，然后使用对用的定位模型来确定物体的位置。
3. 根据分类的分数可以选出k个候选的patch
4. 对patch进行合并

<p align="center"><img src="http://i.imgur.com/SuoCSxl.png" width="600" ></p>

## ***快速滑动窗口***

也就是***在最后一个pool层上做滑动窗口***。
本质上，***在pool做小步长的滑动窗口和在原始图片上做大步长的滑动窗口，从[识别效果]()上来说几乎是等价的***，但是在效率上存在极大的差别——所以这里的“快速”本质上是利用了CNN卷积层和下采样层的空间对应关系来减少计算。

<p align="center"><img src="http://i.imgur.com/QHPlEo3.png" width="600" ></p>

## ***其他一些注意点***

对于分类模型，训练只使用单个尺度（221＊221）进行训练，测试时候不改变网络架构，却是***使用多个尺度进行输入的——于是导致了网络的输出由1维变成了2维（其实就是滑动窗口）***

训练分类模型只是使用了***单尺度***，但后面训练定位模型时候用到了多尺度，个人认为是为了增加样本——因为训练1k个定位模型的话，每类的样本太少了。

本文提出的“***测试时实现多视角多尺度***”方法，其本质上就是***滑动窗口——多视角≈滑动到不同的位置，多视角≈大小不一的窗口***。只是利用CNN的固有属性以及一些技巧，减少了测试时候的运算量。
