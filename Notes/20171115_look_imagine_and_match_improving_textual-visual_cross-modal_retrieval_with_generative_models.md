# [look, imagine and match: improving textual-visual cross-modal retrieval with generative models](https://arxiv.org/abs/1711.06420)

# 所见所想所找:利用生成模型增强跨模态图文检索
本文与新加坡南洋理工大学、莱斯大学合作完成。互联网存在大量的文本和图像数据，因此，图片和文本的交叉检索就自然成为一个非常重要的应用。对于跨模态图文检索这样一个问题，不仅仅需要计算机理解图像同时要理解文本，所以这是一个计算机视觉和自然语言处理结合的研究方向。常见的方法是对文本和图像进行特征提取，然后对提取得到的高层次特征进行匹配。然而，这种方法只考虑了全局的特征，而没有考虑的局部的匹配。实际中，图片检索文本时，我们不仅仅要求文本的意思近似，同时我们还希望句子尽可能匹配；类似，文本检索图片时，我们希望检索到的图片和查询的文本表达相似，而且希望检索到的图片和预期的图片有很高的局部相似度。基于这种思想，我们提出了一种基于生成模型的跨模态检索模型：所看所想所找。所看，指的是我们看并理解了图片或文字；所想，我们通过生成模型生成可能的匹配结果，这个过程也类似“想象”；所找，我们结合所看和所想的特征去找对应的结果。在标准数据库中大量的实验证明，本文提出的跨模态图文检索方法优于目前已有的方法。
