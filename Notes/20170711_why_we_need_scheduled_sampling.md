# Why we need ***Schedule sampling***

I've recently read a paper on a technique called *Scheduled Sampling* for RNNs by Samy Bengio and colleagues at Google.

> Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer (2015): Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks

Google's team developed scheduled sampling as an alternative training procedure to fit RNNs, and they used it in their competition-winning method for image captioning.

# Strictly proper scoring rules

Scoring rules are essentially loss functions for probabilistic models/forecasts. A scoring rule
$$
S(x,Q)
$$

simply measures how bad a probabilistic forecast $Q$ for a variable is in the light of actual observation $x$. In this notation, lower is better. A scoring rule is called strictly proper, if for any $P$, the following holds:
$$
\underset{Q}{\operatorname{argmax}}  \mathbb{E}_{x\sim P}S(x,Q) = P
$$

In other words, if you repeatedly sample observations from some true underlying distribution $P$, then the model $Q$ which minimises expected score is $P$. This means that the scoring rule cannot be fooled and that minimising the expected score yields a consistent estimator for $P$. Because I mention consistency, people may dismiss this as a learning theory argument, but it is not. If you are a Bayesian or a deep learning person with no interest in consistency, a scoring rule being strictly proper simply means that it is safe to use it as a loss function. Anything that's not strictly proper is weird and wrong, it will lead to learning the wrong thing.

This concept is central in unsupervised learning and generative modelling. Unsupervised learning is all about modelling the probability distribution of data, so it's essential that we have loss functions that can measure the discrepancy between our model $Q$, and the true data distribution $P$ in a consistent way.

## log-likelihood
One of the most frequently used strictly proper scoring rule is the logarithmic score:
$$
S(x,Q) = - \log Q(x)
$$
This quantity is also known as the <font style="color:red">negative log-likelihood</font>. Minimising the expected score in an i.i.d scenario yields <font style="color:red">maximum likelihood estimation</font>, which is known to be a consistent estimator and has nice properties.

Often, the likelihood is impossible to evaluate. Luckily, it is not the only strictly proper scoring rule. In the context of generative models people have used the pseudolikelihood, score matching and moment matching, all of which are examples of strictly proper scoring rules.

To recap, any learning method that corresponds to minimising a strictly proper scoring rule is fine, everything else can go horribly wrong, even if we feed it infinite data, it might just learn the wrong thing.

## Scheduled Sampling
After successfully establishing myself as a proper-scoring-rule-nazi, let's talk about scheduled sampling (SS).

SS is a new method to train recurrent neural networks(<font style="color:red">RNNs</font>) to model sequences. I will use character-by-character models of text as an example. Typically, when you train an RNN, you aim to <font style="color:red">minimise the log predictive likelihood in predicting the next character in each training sentence, given the prefix string of previous characters</font>. This can be thought of as a special case of <font style="color:red">maximum likelihood learning</font>, and is all fine, you can actually do this properly <font style="color:red">without approximations</font>.


After training, you use the RNN to <font style="color:red">generate sample sentences in a recursive fashion</font>: assuming you've already generated $n$ characters, you feed that prefix into the RNN, and ask it to predict the $n+1$ st character. The $n+1$ st character is then added to the prefix to predict the $n+2$ th character, and so on.

The authors say <u><b><font style="color:red">there is a disconnect between how the model is trained (it's always fed real data) and how it's used (it's always fed synthetic data generated by itself). This, they argue, leads to the RNN being unable to recover from its own mistakes early on in the sentence.</font></b></u>

To address this, the authors propose an <font style="color:red">alternative training strategy</font>, where every once in a while, the network is given its <u><font style="color:red">own synthetic data instead of real data at training time</font></u>. More specifically, for each character in the training sentences, <b><u>we flip a coin to decide whether we feed the character from the real training sentence, or whether to feed the model's own prediction as to what that character would have been</u></b>. <font style="color:red">The authors claim this makes the model more robust to recovering from mistakes, which is probably true. (see the figure below)</font>

<center><img src="https://i.ytimg.com/vi/agA-rc71Uec/maxresdefault.jpg" width="400"></img></center>

## case study: sequence of two variables
For sake of simplicity, let's consider using scheduled sampling to learn the joint distribution of a sequence of just two random variables. This is probably the simplest (shortest) time series I can think of. So SS in this case works as follows:

1. For each datapoint train the network to predict the real $x_1$.
2. Then we <font style="color:red">flip a coin</font> to decide whether to keep $x_1$ from the datapoint, or to replace it with a sample from the model $Q_{x_1}$.
3. Then we train $Q_{x_2|x_1}$ on the $(x_1,x_2)$ pair obtained this way.


The scoring rule for selective sampling looks something like this:
$$
S(Q_{x_1,x_2},(x_1,x_2)) = - (1 - \epsilon) [ \mathbb{E}_{z \sim Q_{x_1}} \log Q_{x_2 \vert x_1}(x_2 \vert z) + \log Q_{x_1}(x_1)] - \epsilon \log Q_{x_2 , x_1}(x_1,x_2),
$$
where $\epsilon$ is the probability with wich the true $x_1$ is used.

<b><u><font style="color:red">The authors suggest starting training with $\epsilon=1$ and annealing it so that by the end of the training $\epsilon=0$</font></u></b>. So as far as the eventual optimum of SS is concerned, we only have to focus on what the first term of the scoring rule does. The second term is the good old log-likelihood so we know that part works.

After some math, one can show that scheduled sampling with a fixed $\epsilon$ minimises the following divergence between the true $P$ and the model $Q$:
$$
D_{SS}[P\|Q] = KL[P_{x_1}\|Q_{x_1}] + (1-\epsilon) \mathbb{E}_{z\sim Q_{x_1}} KL[P_{x_2}\|Q_{x_2\vert x_1=z}] + \epsilon KL[P_{x_2\vert x_1}\|Q_{x_2\vert x_1}]
$$

Now, if $\epsilon=1$, we recover the Kullback-Leibler divergence between the joint $P_{x1,x2}$ and $Q_{x1,x2}$, which is what we expect as it corresponds to maximum likelihood estimation. However, as $\epsilon$ is annealed to $0$, the objective function is somewhat strange, whereby the conditional distribution $Q_{x_2\vert x_1}$ is pushed to model the marginal distribution $P_{x_2}$, instead of $P_{x_2|x_1}$ as one would expect. One can therefore see that the factorised $Q^{\ast} = P_{x_1}P_{x_2}$ minimises this objective function.


# what this means for text modelling

Extrapolating from the two variable case to longer sequences, one can see that the <font style="color:red">scheduled sampling objective would fail if minimised properly until convergence</font>. Consider the case when the $\epsilon\approx 0$ stage is reached in the annealing schedule. Now consider what the RNN has to do to predict the $n$-th character in a string during training. It is fed a random prefix string that was generated by itself but never seen any real data. <font style="color:red">Then the RNN has to give a probabilistic forecast of what the $n$ th character in the training sentence is, having seen none of the previous characters in the sentence.</font>

<b><u>The optimal model that minimises this objective would completely ignore all the characters in the sentence so far</u></b>, but keep a simple linear counter that indexes where it is within the sentence. Then it would emit a character from an index-specific marginal distribution of characters. This is the equivalent of the factorised trivial solution above.

Yes, such a model would be better at "<font style="color:red">recovering from its own mistakes</font>", because at every character it would start independently from what it has generated so far. <font style="color:red">But this is at the cost of paying no attention whatsoever as to what the prefix of the sentence was</font>. <b><font style="color:red">I believe the reason why this trivial behaviour was not observed in the paper is that the authors did not run the optimisation until convergence, and did not implement the full gradient of the objective function, as they discuss in the paper.</b></font>

# Constructive part of criticism
## What to do instead of SS?

So the <font style="color:red">observed problem was that RNNs trained via maximum likelihood are unable to recover from their own mistakes early on in a sentence, when they are used to generate</font>.

> The main reason for the observed problem is that the log-likelihood is a [local scoring rule](http://arxiv.org/pdf/1101.5011.pdf)

The local property of scoring rules means that at training time we only ever evaluate the model $Q$ on actually observed datapoints. So if the RNN is faced with a prefix subsequence that was not in the dataset, God knows what it's going to complete that sentence with.

The proper (shall I say strictly proper) way to fix this issue is to <font style="color:red">use a non-local strictly proper scoring rule</font>. Luckily, local proper scoring rules are actually pretty rare, so we can use a number of alternatives that should certainly improve things. For example, maximum mean discrepancy as in [Generative Moment Matching Networks](http://arxiv.org/abs/1502.02761) would be an excellent starting point. This is a version of [adversarial training](http://arxiv.org/abs/1406.2661), and although it may be a pain to implement and to get it working, this may be the right approach for training RNNs with good out-of-sample performance as well.

> bottom line: what is a good generative model?

The main message here is that we have to be clear about <font style="color:red">how we evaluate probabilistic models</font>. Very often, generative models are evaluated on the visual or perceptual quality of the samples they generate, rather than on objective terms of how well they model the distribution we wanted them to model. We are often looking for realistic samples, but not too realistic so we can still argue the model generalises well. But I think this practice is wrong, and is not sufficient to understand what really is going on or to detect overfitting. The main point is: being able to generate nice samples is not the same as being a good generative model in the probabilistic sense.
